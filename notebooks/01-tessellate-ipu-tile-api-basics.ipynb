{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d7b882e",
   "metadata": {},
   "source": [
    "Copyright (c) 2023 Graphcore Ltd. All rights reserved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836513db",
   "metadata": {},
   "source": [
    "# Tessellate IPU - Basics of tile mapping on tensors and vertex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cf6f02",
   "metadata": {},
   "source": [
    "The IPU is a highly parallel AI accelerator with 1472 independent IPU-cores (also called IPU tiles) connected with an all-to-all IPU-exchange.\n",
    "Each IPU-tile has 6 independent program threads and 640KB of local SRAM available.\n",
    "\n",
    "**Tessellate IPU** is a library exposing low-level IPU programming primitives in Python, allowing users to take full advantage of the IPU's unique architecture and features. In this tutorial notebook, we present the basics of the Tessellate IPU API, showing how to:\n",
    "\n",
    "* Shard tensors/arrays between IPU tiles using `tile_put_replicated` and `tile_put_sharded`;\n",
    "* Map an IPU vertex (i.e. base function) over a sharded tensor using `tile_map`;\n",
    "* Micro-benchmark Tessellate IPU functions by capturing hardware cycle counts;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfd8c13",
   "metadata": {},
   "source": [
    "## Dependencies and configuration\n",
    "\n",
    "Install JAX experimental for IPU & Tessellate IPU library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f57462b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q jax==0.3.16+ipu jaxlib==0.3.15+ipu.sdk320 -f https://graphcore-research.github.io/jax-experimental/wheels.html\n",
    "%pip install -q git+https://github.com/graphcore-research/tessellate-ipu.git@main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67994c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using IPU model\n",
      "Platform=ipu\n",
      "Number of devices=1\n",
      "IpuDevice(id=0, num_tiles=8, version=ipu2)\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "\n",
    "USE_IPU_MODEL = False\n",
    "if USE_IPU_MODEL:\n",
    "  print('Using IPU model')\n",
    "  from jax.config import config\n",
    "  config.FLAGS.jax_ipu_use_model = True\n",
    "  config.FLAGS.jax_ipu_model_num_tiles = 8\n",
    "\n",
    "# Check IPU hardware configuration\n",
    "print(f\"Platform={jax.default_backend()}\")\n",
    "print(f\"Number of devices={jax.device_count()}\")\n",
    "\n",
    "devices = jax.devices()\n",
    "print(\"\\n\".join([str(d) for d in devices]))\n",
    "device0 = jax.devices()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7316e8b",
   "metadata": {},
   "source": [
    "## Tile tensor sharding in Tessellate IPU\n",
    "\n",
    "Prior to doing any compute on IPU tiles, one needs to decide how to shard the data across tiles.  In normal usage, our frameworks (PopTorch, JAX, Tensorflow) will automatically decide on the optimal mapping of ML workloads using the Poplar compiler.\n",
    "\n",
    "Tessellate IPU provides two primitives to allow the user to control this mapping directly:\n",
    "* `tile_put_sharded`: Shard a tensor over the first axis between a set of tiles;\n",
    "* `tile_put_replicated`: Replicate a tensor on a set of tiles;\n",
    "\n",
    "Both methods return a `TileShardedArray` Python object, which wraps a common JAX array and explicit (static) tile mapping.\n",
    "A `TileShardedArray` tensor is always sharded over the first axis, and on-tile shards are contiguous in memory.\n",
    "Reshaping or premuting the indices of such a tensor will use the all-to-all IPU exchange to efficiently rearrange the data across tiles. \n",
    "\n",
    "Here is an example on how to use these two methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b69110c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First time compilation may take up to a minute... run the next cell to see the results\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import numpy as np\n",
    "\n",
    "from tessellate_ipu import tile_put_replicated, tile_put_sharded\n",
    "\n",
    "data = np.random.rand(3, 5).astype(np.float32)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def compute_fn(data):\n",
    "    # Shard data on tiles (0, 1, 3)\n",
    "    t0 = tile_put_sharded(data, (0, 1, 3))\n",
    "    # Replicate data on tiles (1, 3)\n",
    "    t1 = tile_put_replicated(data, (1, 3))\n",
    "    return t0, t1\n",
    "\n",
    "print('First time compilation may take up to a minute... run the next cell to see the results')\n",
    "t0, t1 = compute_fn(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4ea088f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor `t0` shape (3, 5) and tile mapping (0, 1, 3).\n",
      "Tensor `t1` shape (2, 3, 5) and tile mapping (1, 3).\n"
     ]
    }
   ],
   "source": [
    "# `t0` has the same shape, just sharded between tiles.\n",
    "print(f\"Tensor `t0` shape {t0.shape} and tile mapping {t0.tiles}.\")\n",
    "# `t1` has an additional replication axis.\n",
    "print(f\"Tensor `t1` shape {t1.shape} and tile mapping {t1.tiles}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49208128",
   "metadata": {},
   "source": [
    "`TileShardedArray` tensors support the basic array API, such as slicing and indexing. Note that an error will be raised if the slicing of a tensor would result into non-contiguous on tile shards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89793d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor `t3` shape (2, 3) and tile mapping (1, 3).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(DeviceArray([[0.7707571 , 0.78871965, 0.47292888],\n",
       "              [0.01404075, 0.296928  , 0.62824464]], dtype=float32),\n",
       " array([[0.7707571 , 0.78871965, 0.47292888],\n",
       "        [0.01404075, 0.296928  , 0.62824464]], dtype=float32))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t3 = t0[1:, 2:5] # t0 is on tiles (0,1,3), t3 will be on (1,3)\n",
    "print(f\"Tensor `t3` shape {t3.shape} and tile mapping {t3.tiles}.\")\n",
    "# Extract the underlying tensor/array, or convert to a Numpy array\n",
    "t3.array, np.array(t3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525771e3",
   "metadata": {},
   "source": [
    "## Tile map a vertex using Tessellate IPU\n",
    "\n",
    "Once tensors have been sharded across IPU tiles, users can map computation kernels (called IPU vertices) to these arrays.\n",
    "Tessellate IPU supports out of the box (part of) [JAX LAX operations](https://jax.readthedocs.io/en/latest/jax.lax.html) by mapping them to pre-existing Graphcore Poplar SDK optimized vertices, allowing to perform basic operations in a couple of lines.\n",
    "\n",
    "In the following example, we write a simple `broadcast_add` using Tessellate IPU. In this broadcast operation, the left hand term is sharded across a collection of tiles whereas the right hand term is broadcasted (i.e. replicated on all tiles). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7f38e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out=TileShardedArray(array=DeviceArray([[[1.4182293 , 1.4438568 ],\n",
      "              [1.2700567 , 1.167884  ],\n",
      "              [0.63169944, 1.6121812 ],\n",
      "              [1.2657567 , 1.7713295 ],\n",
      "              [1.4050148 , 0.09321211]],\n",
      "\n",
      "             [[1.6221857 , 0.90384614],\n",
      "              [0.6746084 , 0.8345685 ],\n",
      "              [0.6444476 , 1.4079747 ],\n",
      "              [1.203553  , 1.3113172 ],\n",
      "              [0.84221095, 0.97481364]],\n",
      "\n",
      "             [[1.4760879 , 1.3465476 ],\n",
      "              [1.4654338 , 0.5735177 ],\n",
      "              [1.2354691 , 1.0445356 ],\n",
      "              [1.0398397 , 1.918472  ],\n",
      "              [0.7950085 , 0.9415547 ]]], dtype=float32), tiles=(0, 1, 2))\n",
      "lhs_data + rhs_data - out=array([[[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]],\n",
      "\n",
      "       [[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]],\n",
      "\n",
      "       [[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.lax\n",
    "import numpy as np\n",
    "\n",
    "from tessellate_ipu import tile_map, tile_put_replicated, tile_put_sharded\n",
    "\n",
    "lhs_data = np.random.rand(3, 5, 2).astype(np.float32)\n",
    "rhs_data = np.random.rand(5, 2).astype(np.float32)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def broadcast_add(lhs, rhs):\n",
    "    # LHS is size TxMxNx... split onto first T tiles\n",
    "    T, M, N = lhs.shape\n",
    "    tiles = tuple(range(len(lhs)))\n",
    "\n",
    "    # Shard lhs on tiles (0, 1, ..., T)\n",
    "    lhs = tile_put_sharded(lhs, tiles)\n",
    "    # Replicate rhs on tiles (0, 1, ..., T)\n",
    "    rhs = tile_put_replicated(rhs, tiles)\n",
    "\n",
    "    # Map Poplar optimized `add` vertex to the sharded data.\n",
    "    out = tile_map(jax.lax.add_p, lhs, rhs)\n",
    "    return out\n",
    "\n",
    "\n",
    "out = broadcast_add(lhs_data, rhs_data)\n",
    "\n",
    "print(f\"{out=}\")\n",
    "print(f\"{lhs_data + rhs_data - out=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d7a726",
   "metadata": {},
   "source": [
    "As seen above, `tile_map` will always return `TileShardedArray` objects, with tile mapping deduced from inputs. It will as well check that inputs are `TileShardedArray` instances. Since the `TileShardedArray` class insures that data is already sharded on IPU tiles in a contiguous form, `tile_map` has no performance overhead (i.e. no implicit on-tile-copy or tile exchange).\n",
    "\n",
    "**Note:** Tessellate IPU will always check that the tile mapping is consistent, and will raise an error if not. As the goal of Tessellate IPU is to provide a way to write performant & efficient algorithms directly in Python, implicit exchange between IPU tiles (or on-tile-copy) is not allowed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04d1c08f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good! Raised exception:\n",
      "Inconsistent tile mapping between input arrays: (0, 1, 2) vs (1, 2, 3).\n"
     ]
    }
   ],
   "source": [
    "@jax.jit\n",
    "def broadcast_add_error(lhs, rhs):\n",
    "    lhs = tile_put_sharded(lhs, range(len(lhs)))\n",
    "    rhs = tile_put_replicated(rhs, range(1, len(lhs) + 1))\n",
    "    out = tile_map(jax.lax.add_p, lhs, rhs)\n",
    "    return out\n",
    "\n",
    "\n",
    "# Raise `ValueError`: inconsistent tile mapping!\n",
    "try:\n",
    "    broadcast_add_error(lhs_data, rhs_data)\n",
    "except Exception as e:\n",
    "    print(f\"Good! Raised exception:\\n{e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0d361d",
   "metadata": {},
   "source": [
    "TessellateIPU code written using standard JAX LAX primitives remains fully compatible with **other backends (CPU, GPU, TPU)**. `tile_put_sharded` is a no-op on other backends and `tile_put_replicated` is a simple `concatenate` of the input tensor. Finally `tile_map` is translated into a standard JAX `vmap` call.\n",
    "\n",
    "As a consequence, one can run the exact same function on JAX CPU backend:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e20f0dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: TileShardedArray(array=DeviceArray([[[1.4182293 , 1.4438568 ],\n",
      "              [1.2700567 , 1.167884  ],\n",
      "              [0.63169944, 1.6121812 ],\n",
      "              [1.2657567 , 1.7713295 ],\n",
      "              [1.4050148 , 0.09321211]],\n",
      "\n",
      "             [[1.6221857 , 0.90384614],\n",
      "              [0.6746084 , 0.8345685 ],\n",
      "              [0.6444476 , 1.4079747 ],\n",
      "              [1.203553  , 1.3113172 ],\n",
      "              [0.84221095, 0.97481364]],\n",
      "\n",
      "             [[1.4760879 , 1.3465476 ],\n",
      "              [1.4654338 , 0.5735177 ],\n",
      "              [1.2354691 , 1.0445356 ],\n",
      "              [1.0398397 , 1.918472  ],\n",
      "              [0.7950085 , 0.9415547 ]]], dtype=float32), tiles=(0, 1, 2)) on device: TFRT_CPU_0\n"
     ]
    }
   ],
   "source": [
    "# JIT function on CPU backend.\n",
    "broadcast_add_cpu = jax.jit(broadcast_add, device=jax.devices(\"cpu\")[0])\n",
    "# Running on CPU.\n",
    "out_cpu = broadcast_add_cpu(lhs_data, rhs_data)\n",
    "# Check data & device.\n",
    "print(f\"Output: {out_cpu} on device: {out_cpu.array.device()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a186fa0",
   "metadata": {},
   "source": [
    "# Micro-benchmarking in Tessellate IPU\n",
    "\n",
    "When writing performant algorithms, micro-benchmarking is a recommended practice to ensure quick progress and no performance regression. Tessellate IPU is fully compatible with [Graphcore Popvision tools](https://www.graphcore.ai/developer/popvision-tools), but also provides a way to directly measure IPU hardware cycle count with the Python function `ipu_cycle_count`.\n",
    "\n",
    "**Note:** Cycle count is not available on the IPU model simulator, `ipu_cycle_count` will always return a zeroed tensor on the latter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3519b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start cycle count: TileShardedArray(array=DeviceArray([[0, 0],\n",
      "             [0, 0],\n",
      "             [0, 0]], dtype=uint32), tiles=(0, 1, 2)) (3, 2)\n",
      "End cycle count: TileShardedArray(array=DeviceArray([[0, 0],\n",
      "             [0, 0],\n",
      "             [0, 0]], dtype=uint32), tiles=(0, 1, 2)) (3, 2)\n"
     ]
    }
   ],
   "source": [
    "from tessellate_ipu import ipu_cycle_count\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def broadcast_add(lhs, rhs):\n",
    "    # Tiles to split the workload on.\n",
    "    tiles = tuple(range(len(lhs)))\n",
    "    # Shard lhs on tiles (0, 1, ..., N)\n",
    "    lhs = tile_put_sharded(lhs, tiles)\n",
    "    # Replicate rhs on tiles (0, 1, ..., N)\n",
    "    rhs = tile_put_replicated(rhs, tiles)\n",
    "\n",
    "    # Cycle count after `(lhs,rhs)` are sharded.\n",
    "    lhs, rhs, start = ipu_cycle_count(lhs, rhs)\n",
    "    # Map Poplar optimized `add` vertex to the sharded data.\n",
    "    out = tile_map(jax.lax.add_p, lhs, rhs)\n",
    "    # Cycle count after `out` is computed.\n",
    "    out, end = ipu_cycle_count(out)\n",
    "    return out, start, end\n",
    "\n",
    "\n",
    "_, start, end = broadcast_add(lhs_data, rhs_data)\n",
    "\n",
    "print(\"Start cycle count:\", start, start.shape)\n",
    "print(\"End cycle count:\", end, end.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9886bfcd",
   "metadata": {},
   "source": [
    "The function `ipu_cycle_count` returns raw cycle counts on every IPU-tile, directly measured by the hardware. Note that `ipu_cycle_count` takes input arguments and return them unchanged in order to provide control flow information to XLA and Poplar compilers (i.e. measure cycle counts after these tensors have been computed).\n",
    "\n",
    "\n",
    "Tessellate provides the raw values returned by IPU C++ [get_scount](https://docs.graphcore.ai/projects/poplar-api/en/3.3.0/ipu_intrinsics/ipu_builtins.html#ipu-functionality-and-memory) intrinsics, hence `start` and `end` tensors have `uint32x2` as datatype.\n",
    "As shown below, the raw cycle count can be easily translated into time performance figures.\n",
    "Please note timing measured this way will differ massively from simple Python benchmarking of the function `broadcast_add`, as the latter will also include any JAX overhead, host to/from IPU communications and IPU tile exchange!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33142bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tile map `add` execution time (micro-seconds): 0.0\n"
     ]
    }
   ],
   "source": [
    "def cycle_count_to_timing(start, end, ipu_device):\n",
    "    \"\"\"Convert raw cycle count into timing.\"\"\"\n",
    "    # Lower & upper bounds on cycle count.\n",
    "    start = np.array(start).view(dtype=np.int64)\n",
    "    end = np.array(end).view(dtype=np.int64)\n",
    "    cycle_count_max = np.max(end - start)\n",
    "    timing = cycle_count_max / ipu_device.tile_clock_frequency\n",
    "    return timing\n",
    "\n",
    "timing = cycle_count_to_timing(start, end, device0)\n",
    "print(\"Tile map `add` execution time (micro-seconds):\", timing * 1e6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f5b427",
   "metadata": {},
   "source": [
    "### Cycle count & IPU tile parallelism\n",
    "\n",
    "Let's demonstrate IPU tile parallelism in a simple way by using hardware cycle count: `broadcast_add` performance should (roughly) independent of the input first axis size, as the workload is split uniformly between tiles.\n",
    "Note that timing can vary slightly depending on the number of tiles used as the latter are not synchronized when running compute-independent workloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31523a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_IPU_MODEL:\n",
    "  # Number of tiles to use.\n",
    "  num_tiles_list = [4, 16, 128, 1024]\n",
    "\n",
    "\n",
    "  def ipu_benchmark(num_tiles):\n",
    "      \"\"\"IPU benchmarking, splitting the workload over a collection of tiles.\"\"\"\n",
    "      # Workload size per tile.\n",
    "      wsize = 1024\n",
    "      lhs_data = np.random.rand(num_tiles, wsize).astype(np.float32)\n",
    "      rhs_data = np.random.rand(wsize).astype(np.float32)\n",
    "\n",
    "      _, start, end = broadcast_add(lhs_data, rhs_data)\n",
    "      timing = cycle_count_to_timing(start, end, device0)\n",
    "      return timing\n",
    "\n",
    "\n",
    "  benchmarks = [ipu_benchmark(N) * 1e6 for N in num_tiles_list]\n",
    "\n",
    "  # (Roughly) constant timing independently of the number of tiles used.\n",
    "  print(\"Benchmark timing (us):\", benchmarks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
