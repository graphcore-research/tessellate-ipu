{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d7b882e",
   "metadata": {},
   "source": [
    "Copyright (c) 2023 Graphcore Ltd. All rights reserved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836513db",
   "metadata": {},
   "source": [
    "# TessellateIPU - Basics of Tile Mapping on Tensors and Vertices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cf6f02",
   "metadata": {},
   "source": [
    "The IPU is a highly parallel AI accelerator with 1,472 independent cores (also called IPU tiles) connected with an all-to-all IPU-exchange.\n",
    "Each IPU tile has six independent program threads and 640 KB of local SRAM.\n",
    "\n",
    "**TessellateIPU** is a library exposing low-level IPU programming primitives in Python, allowing users to take full advantage of the IPU's unique architecture and features. In this tutorial notebook, we present the basics of the TessellateIPU API, showing how to:\n",
    "\n",
    "* Shard tensors across IPU tiles using `tile_put_replicated` and `tile_put_sharded`;\n",
    "* Map an IPU vertex (computational kernel) over a sharded tensor using `tile_map`;\n",
    "* Micro-benchmark TessellateIPU functions by capturing hardware cycle counts;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfd8c13",
   "metadata": {},
   "source": [
    "## Dependencies and configuration\n",
    "\n",
    "Install the JAX experimental for IPU and TessellateIPU libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f57462b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q jax==0.3.16+ipu jaxlib==0.3.15+ipu.sdk320 -f https://graphcore-research.github.io/jax-experimental/wheels.html\n",
    "%pip install -q git+https://github.com/graphcore-research/tessellate-ipu.git@main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67994c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Platform=ipu\n",
      "Number of devices=4\n",
      "IpuDevice(id=0, num_tiles=1472, version=ipu2)\n",
      "IpuDevice(id=1, num_tiles=1472, version=ipu2)\n",
      "IpuDevice(id=2, num_tiles=1472, version=ipu2)\n",
      "IpuDevice(id=3, num_tiles=1472, version=ipu2)\n"
     ]
    }
   ],
   "source": [
    "from jax.config import config\n",
    "\n",
    "USE_IPU_MODEL = False\n",
    "if USE_IPU_MODEL:\n",
    "    print(\"Using IPU model\")\n",
    "    config.FLAGS.jax_ipu_use_model = True\n",
    "    config.FLAGS.jax_ipu_model_num_tiles = 8\n",
    "\n",
    "# Set to true to see when JAX recompiles - see README for other flags\n",
    "config.update(\"jax_log_compiles\", False)\n",
    "\n",
    "import jax\n",
    "\n",
    "# Check IPU hardware configuration\n",
    "assert jax.default_backend() == \"ipu\"\n",
    "print(f\"Platform={jax.default_backend()}\")\n",
    "print(f\"Number of devices={jax.device_count()}\")\n",
    "\n",
    "devices = jax.devices()\n",
    "print(\"\\n\".join([str(d) for d in devices]))\n",
    "device0 = jax.devices()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7316e8b",
   "metadata": {},
   "source": [
    "## Tile tensor sharding in TessellateIPU\n",
    "\n",
    "Prior to doing any compute on IPU tiles, you need to decide how to shard the data across tiles.  In normal usage, our IPU frameworks (PyTorch, TensorFlow, and JAX) will automatically decide on the optimal mapping of ML workloads using the Poplar compiler.\n",
    "\n",
    "TessellateIPU provides two primitives to allow the user to control this mapping directly:\n",
    "* `tile_put_sharded`: Shards a tensor over the first axis between a set of tiles.\n",
    "* `tile_put_replicated`: Replicates a tensor on a set of tiles.\n",
    "\n",
    "Both methods return a `TileShardedArray` Python object, which wraps a common JAX array and explicit (static) tile mapping.\n",
    "A `TileShardedArray` tensor is always sharded over the first axis, and on-tile shards are contiguous in memory.\n",
    "Reshaping or permuting the indices of such a tensor will use the all-to-all IPU exchange to efficiently rearrange the data across tiles. \n",
    "\n",
    "Here is an example showing how to use these two methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b69110c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First time compilation may take up to a minute... run the next cell to see the results\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from tessellate_ipu import tile_put_replicated, tile_put_sharded\n",
    "\n",
    "data = np.random.rand(3, 5).astype(np.float32)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def compute_fn(data):\n",
    "    # Shard data on tiles (0, 1, 3)\n",
    "    t0 = tile_put_sharded(data, (0, 1, 3))\n",
    "    # Replicate data on tiles (1, 3)\n",
    "    t1 = tile_put_replicated(data, (1, 3))\n",
    "    return t0, t1\n",
    "\n",
    "\n",
    "print(\"First time compilation may take up to a minute... run the next cell to see the results\")\n",
    "t0, t1 = compute_fn(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4ea088f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor `t0` shape (3, 5) and tile mapping (0, 1, 3).\n",
      "Tensor `t1` shape (2, 3, 5) and tile mapping (1, 3).\n"
     ]
    }
   ],
   "source": [
    "# `t0` has the same shape, just sharded between tiles.\n",
    "print(f\"Tensor `t0` shape {t0.shape} and tile mapping {t0.tiles}.\")\n",
    "# `t1` has an additional replication axis.\n",
    "print(f\"Tensor `t1` shape {t1.shape} and tile mapping {t1.tiles}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49208128",
   "metadata": {},
   "source": [
    "`TileShardedArray` tensors support the basic array API, such as slicing and indexing. Note that an error will be raised if the slicing of a tensor will result in non-contiguous on-tile shards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89793d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor `t3` shape (2, 3) and tile mapping (1, 3).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(DeviceArray([[0.97064054, 0.18166828, 0.9435759 ],\n",
       "              [0.6613753 , 0.66417444, 0.22481899]], dtype=float32),\n",
       " array([[0.97064054, 0.18166828, 0.9435759 ],\n",
       "        [0.6613753 , 0.66417444, 0.22481899]], dtype=float32))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t3 = t0[1:, 2:5]  # t0 is on tiles (0,1,3), t3 will be on (1,3)\n",
    "print(f\"Tensor `t3` shape {t3.shape} and tile mapping {t3.tiles}.\")\n",
    "# Extract the underlying tensor/array, or convert to a Numpy array\n",
    "t3.array, np.array(t3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525771e3",
   "metadata": {},
   "source": [
    "## Tile map a vertex using TessellateIPU\n",
    "\n",
    "Once tensors have been sharded across IPU tiles, you can then map computation kernels (called IPU vertices) over these arrays.\n",
    "TessellateIPU supports out-of-the-box (part of) [JAX LAX operations](https://jax.readthedocs.io/en/latest/jax.lax.html) by mapping them to pre-existing Graphcore Poplar SDK optimized vertices, allowing you to perform basic operations in a couple of lines.\n",
    "\n",
    "In the following example, we write a simple `broadcast_add` operation using TessellateIPU. In this broadcast operation, the left hand term is sharded across a collection of tiles whereas the right hand term is broadcasted (so replicated on all tiles). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7f38e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out=TileShardedArray(array=DeviceArray([[[1.5220014 , 0.8861085 ],\n",
      "              [1.1387997 , 0.5535539 ],\n",
      "              [0.76053864, 1.6265361 ],\n",
      "              [0.4890347 , 1.5736914 ],\n",
      "              [1.1018195 , 0.8729285 ]],\n",
      "\n",
      "             [[1.7426736 , 0.7907614 ],\n",
      "              [1.9202613 , 0.5467136 ],\n",
      "              [0.9664666 , 1.108542  ],\n",
      "              [0.5841475 , 1.7479924 ],\n",
      "              [0.5022569 , 0.98529065]],\n",
      "\n",
      "             [[1.4763346 , 0.86958444],\n",
      "              [1.87937   , 0.75770867],\n",
      "              [0.7525834 , 1.4304872 ],\n",
      "              [0.8697836 , 1.8902819 ],\n",
      "              [1.13012   , 0.7493061 ]]], dtype=float32), tiles=(0, 1, 2))\n",
      "lhs_data + rhs_data - out=array([[[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]],\n",
      "\n",
      "       [[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]],\n",
      "\n",
      "       [[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.lax\n",
    "import numpy as np\n",
    "\n",
    "from tessellate_ipu import tile_map, tile_put_replicated, tile_put_sharded\n",
    "\n",
    "lhs_data = np.random.rand(3, 5, 2).astype(np.float32)\n",
    "rhs_data = np.random.rand(5, 2).astype(np.float32)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def broadcast_add(lhs, rhs):\n",
    "    # LHS is size TxMxNx... split onto first T tiles\n",
    "    T, M, N = lhs.shape\n",
    "    tiles = tuple(range(len(lhs)))\n",
    "\n",
    "    # Shard lhs on tiles (0, 1, ..., T)\n",
    "    lhs = tile_put_sharded(lhs, tiles)\n",
    "    # Replicate rhs on tiles (0, 1, ..., T)\n",
    "    rhs = tile_put_replicated(rhs, tiles)\n",
    "\n",
    "    # Map Poplar optimized `add` vertex to the sharded data.\n",
    "    out = tile_map(jax.lax.add_p, lhs, rhs)\n",
    "    return out\n",
    "\n",
    "\n",
    "out = broadcast_add(lhs_data, rhs_data)\n",
    "\n",
    "print(f\"{out=}\")\n",
    "print(f\"{lhs_data + rhs_data - out=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d7a726",
   "metadata": {},
   "source": [
    "As seen above, `tile_map` will always return `TileShardedArray` objects, with tile mapping deduced from inputs. It will also check that inputs are `TileShardedArray` instances. Since the `TileShardedArray` class insures that data is already sharded on IPU tiles in a contiguous form, `tile_map` has no performance overhead (so there is no implicit on-tile copy or tile exchange).\n",
    "\n",
    "**Note:** TessellateIPU will always check that the tile mapping is consistent, and will raise an error if it isn't. As the goal of TessellateIPU is to provide a way to write performant and efficient algorithms directly in Python, implicit exchanges between IPU tiles (or on-tile copies) are not allowed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04d1c08f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good! Raised exception:\n",
      "Inconsistent tile mapping between input arrays: (0, 1, 2) vs (1, 2, 3).\n"
     ]
    }
   ],
   "source": [
    "@jax.jit\n",
    "def broadcast_add_error(lhs, rhs):\n",
    "    lhs = tile_put_sharded(lhs, range(len(lhs)))\n",
    "    rhs = tile_put_replicated(rhs, range(1, len(lhs) + 1))\n",
    "    out = tile_map(jax.lax.add_p, lhs, rhs)\n",
    "    return out\n",
    "\n",
    "\n",
    "# Raise `ValueError`: inconsistent tile mapping!\n",
    "try:\n",
    "    broadcast_add_error(lhs_data, rhs_data)\n",
    "except Exception as e:\n",
    "    print(f\"Good! Raised exception:\\n{e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0d361d",
   "metadata": {},
   "source": [
    "TessellateIPU code written using standard JAX LAX primitives remains fully compatible with **other backends (CPU, GPU, TPU)**. `tile_put_sharded` is a no-op on other backends and `tile_put_replicated` is a simple `concatenate` of the input tensor. Finally `tile_map` is translated into a standard JAX `vmap` call.\n",
    "\n",
    "As a consequence, one can run the identical function on a JAX CPU backend:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e20f0dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: TileShardedArray(array=DeviceArray([[[1.5220014 , 0.8861085 ],\n",
      "              [1.1387997 , 0.5535539 ],\n",
      "              [0.76053864, 1.6265361 ],\n",
      "              [0.4890347 , 1.5736914 ],\n",
      "              [1.1018195 , 0.8729285 ]],\n",
      "\n",
      "             [[1.7426736 , 0.7907614 ],\n",
      "              [1.9202613 , 0.5467136 ],\n",
      "              [0.9664666 , 1.108542  ],\n",
      "              [0.5841475 , 1.7479924 ],\n",
      "              [0.5022569 , 0.98529065]],\n",
      "\n",
      "             [[1.4763346 , 0.86958444],\n",
      "              [1.87937   , 0.75770867],\n",
      "              [0.7525834 , 1.4304872 ],\n",
      "              [0.8697836 , 1.8902819 ],\n",
      "              [1.13012   , 0.7493061 ]]], dtype=float32), tiles=(0, 1, 2)) on device: TFRT_CPU_0\n"
     ]
    }
   ],
   "source": [
    "# JIT function on CPU backend.\n",
    "broadcast_add_cpu = jax.jit(broadcast_add, device=jax.devices(\"cpu\")[0])\n",
    "# Running on CPU.\n",
    "out_cpu = broadcast_add_cpu(lhs_data, rhs_data)\n",
    "# Check data & device.\n",
    "print(f\"Output: {out_cpu} on device: {out_cpu.array.device()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a186fa0",
   "metadata": {},
   "source": [
    "# Micro-benchmarking in TessellateIPU\n",
    "\n",
    "When writing performant algorithms, micro-benchmarking is a recommended practice to ensure quick progress and no performance regression. TessellateIPU is fully compatible with the [Graphcore PopVision tools](https://www.graphcore.ai/developer/popvision-tools), but also provides a way to directly measure IPU hardware cycle count with the Python function `ipu_cycle_count`.\n",
    "\n",
    "**Note:** Cycle count is not available on the IPU model simulator. `ipu_cycle_count` will always return a zeroed tensor on the IPU model simulator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3519b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start cycle count: TileShardedArray(array=DeviceArray([[433085005,         0],\n",
      "             [433085005,         0],\n",
      "             [433085006,         0]], dtype=uint32), tiles=(0, 1, 2)) (3, 2)\n",
      "End cycle count: TileShardedArray(array=DeviceArray([[433085334,         0],\n",
      "             [433085334,         0],\n",
      "             [433085336,         0]], dtype=uint32), tiles=(0, 1, 2)) (3, 2)\n"
     ]
    }
   ],
   "source": [
    "from tessellate_ipu import ipu_cycle_count\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def broadcast_add(lhs, rhs):\n",
    "    # Tiles to split the workload on.\n",
    "    tiles = tuple(range(len(lhs)))\n",
    "    # Shard lhs on tiles (0, 1, ..., N)\n",
    "    lhs = tile_put_sharded(lhs, tiles)\n",
    "    # Replicate rhs on tiles (0, 1, ..., N)\n",
    "    rhs = tile_put_replicated(rhs, tiles)\n",
    "\n",
    "    # Cycle count after `(lhs,rhs)` are sharded.\n",
    "    lhs, rhs, start = ipu_cycle_count(lhs, rhs)\n",
    "    # Map Poplar optimized `add` vertex to the sharded data.\n",
    "    out = tile_map(jax.lax.add_p, lhs, rhs)\n",
    "    # Cycle count after `out` is computed.\n",
    "    out, end = ipu_cycle_count(out)\n",
    "    return out, start, end\n",
    "\n",
    "\n",
    "_, start, end = broadcast_add(lhs_data, rhs_data)\n",
    "\n",
    "print(\"Start cycle count:\", start, start.shape)\n",
    "print(\"End cycle count:\", end, end.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9886bfcd",
   "metadata": {},
   "source": [
    "The function `ipu_cycle_count` returns raw cycle counts on every IPU tile, directly measured by the hardware. Note that `ipu_cycle_count` takes input arguments and returns them unchanged in order to provide control flow information to XLA and Poplar compilers (it measures cycle counts after these tensors have been computed).\n",
    "\n",
    "TessellateIPU provides the raw values returned by IPU C++ [get_scount](https://docs.graphcore.ai/projects/poplar-api/en/3.3.0/ipu_intrinsics/ipu_builtins.html#ipu-functionality-and-memory) intrinsics, and this is why `start` and `end` tensors have type `uint32x2xT`.\n",
    "As shown below, the raw cycle count can be easily translated into time performance figures.\n",
    "Please note timing measured in this way will differ significantly from simple Python benchmarking of the `broadcast_add` function, as the latter will also include any JAX overhead, as well as all communication between the host and the IPU, and IPU tile exchange."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33142bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tile map `add` execution time (micro-seconds): 0.1783783783783784\n"
     ]
    }
   ],
   "source": [
    "def cycle_count_to_timing(start, end, ipu_device):\n",
    "    \"\"\"Convert raw cycle count into timing.\"\"\"\n",
    "    # Lower & upper bounds on cycle count.\n",
    "    start = np.array(start).view(dtype=np.int64)\n",
    "    end = np.array(end).view(dtype=np.int64)\n",
    "    cycle_count_max = np.max(end - start)\n",
    "    timing = cycle_count_max / ipu_device.tile_clock_frequency\n",
    "    return timing\n",
    "\n",
    "\n",
    "timing = cycle_count_to_timing(start, end, device0)\n",
    "print(\"Tile map `add` execution time (micro-seconds):\", timing * 1e6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f5b427",
   "metadata": {},
   "source": [
    "### Cycle count and IPU tile parallelism\n",
    "\n",
    "Let's demonstrate IPU tile parallelism in a simple way by using hardware cycle count. The performance of `broadcast_add` should be (roughly) independent of the size of the first axis of the input, as the workload is split uniformly between tiles.\n",
    "Note that timing can vary slightly depending on the number of tiles used as the tiles are not synchronized when running compute-independent workloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31523a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking on 4 tiles\n",
      "Benchmarking on 16 tiles\n",
      "Benchmarking on 128 tiles\n",
      "Benchmarking on 1024 tiles\n",
      "Benchmark timing (us): [1.0043243243243243, 1.0054054054054054, 1.0054054054054054, 1.0054054054054054]\n"
     ]
    }
   ],
   "source": [
    "if USE_IPU_MODEL:\n",
    "    print(\"Benchmarks don't run on IPUModel\")\n",
    "else:\n",
    "    # Number of tiles to use.\n",
    "    num_tiles_list = [4, 16, 128, 1024]\n",
    "\n",
    "    def ipu_benchmark(num_tiles):\n",
    "        \"\"\"IPU benchmarking, splitting the workload over a collection of tiles.\"\"\"\n",
    "        print(f\"Benchmarking on {num_tiles} tiles\")\n",
    "        # Workload size per tile.\n",
    "        wsize = 1024\n",
    "        lhs_data = np.random.rand(num_tiles, wsize).astype(np.float32)\n",
    "        rhs_data = np.random.rand(wsize).astype(np.float32)\n",
    "\n",
    "        _, start, end = broadcast_add(lhs_data, rhs_data)\n",
    "        timing = cycle_count_to_timing(start, end, device0)\n",
    "        return timing\n",
    "\n",
    "    benchmarks = [ipu_benchmark(N) * 1e6 for N in num_tiles_list]\n",
    "\n",
    "    # (Roughly) constant timing independently of the number of tiles used.\n",
    "    print(\"Benchmark timing (us):\", benchmarks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710bdd70",
   "metadata": {},
   "source": [
    "And that's it: a three-function API showing how to directly map computations onto IPU hardware.\n",
    "\n",
    "You might like to look next at the [IPU Peak Flops](IPU%20Peak%20Flops.ipynb) notebook, or at the [`demo_vertex.py`](../examples/demo/demo_vertex.py) example."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
