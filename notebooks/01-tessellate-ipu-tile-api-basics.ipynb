{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d7b882e",
   "metadata": {},
   "source": [
    "Copyright (c) 2023 Graphcore Ltd. All rights reserved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836513db",
   "metadata": {},
   "source": [
    "# Tessellate IPU - Basics of tile mapping on tensors and vertex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cf6f02",
   "metadata": {},
   "source": [
    "The IPU is a highly parallel architecture with 1472 independent IPU-core (also called IPU tiles) connected with an all-to-all IPU-exchange. Each IPU-tile has 6 independent program threads and 639kB of local SRAM available.\n",
    "\n",
    "**Tessellate IPU** is a library exposing low-level IPU programming primitives in Python, allowing users to take full advantage of the IPU unique architecture and features. In this tutorial notebook, we present the basics of Tessellate IPU API, learning how to:\n",
    "\n",
    "* Shard tensors/arrays between IPU tiles using `tile_put_replicated` and `tile_put_sharded`;\n",
    "* Map an IPU vertex (i.e. base function) on sharded tensor using `tile_map`;\n",
    "* Micro-benchmarks Tessellate IPU functions by capturing hardware cycle counts;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfd8c13",
   "metadata": {},
   "source": [
    "## Dependencies and configuration\n",
    "\n",
    "Install JAX experimental for IPU & Tessellate IPU library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f57462b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q jax==0.3.16+ipu jaxlib==0.3.15+ipu.sdk320 -f https://graphcore-research.github.io/jax-experimental/wheels.html\n",
    "!pip install -q git+https://github.com/graphcore-research/tessellate-ipu.git@main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67994c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "\n",
    "# Uncomment to use IPU model emulator.\n",
    "# from jax.config import config\n",
    "# config.FLAGS.jax_ipu_use_model = True\n",
    "# config.FLAGS.jax_ipu_model_num_tiles = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d34c2d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Platform=ipu\n",
      "Number of devices=4\n",
      "IpuDevice(id=0, num_tiles=1472, version=ipu2)\n",
      "IpuDevice(id=1, num_tiles=1472, version=ipu2)\n",
      "IpuDevice(id=2, num_tiles=1472, version=ipu2)\n",
      "IpuDevice(id=3, num_tiles=1472, version=ipu2)\n"
     ]
    }
   ],
   "source": [
    "# Check IPU hardware configuration\n",
    "print(f\"Platform={jax.default_backend()}\")\n",
    "print(f\"Number of devices={jax.device_count()}\")\n",
    "\n",
    "devices = jax.devices()\n",
    "print(\"\\n\".join([str(d) for d in devices]))\n",
    "d = jax.devices()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c2792a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c7316e8b",
   "metadata": {},
   "source": [
    "## Tile tensor sharding in Tessellate IPU\n",
    "\n",
    "Prior to doing any compute on IPU tiles, one needs to decide how to shard the data between tiles. Poptorch, TensorFlow and JAX frameworks rely on Poplar compiler to automatically decide on the optimal mapping of ML workloads. Tessellate IPU provides two primitives to allow the user to control this mapping directly:\n",
    "* `tile_put_sharded`: Shard a tensor over the first axis between a set of tiles;\n",
    "* `tile_put_replicated`: Replicate a tensor on a set of tiles;\n",
    "\n",
    "Both methods return a `TileShardedArray` Python object, which wraps a common JAX array and explicit (static) tile mapping. By convention, a `TileShardedArray` tensor is always sharded over the first axis, and on-tile shards are contiguous in memory.\n",
    "\n",
    "Here is an example on how to use this two methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b69110c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import numpy as np\n",
    "\n",
    "from tessellate_ipu import tile_put_replicated, tile_put_sharded\n",
    "\n",
    "data = np.random.rand(3, 5).astype(np.float32)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def compute_fn(data):\n",
    "    # Shard data on tiles (0, 1, 3)\n",
    "    t0 = tile_put_sharded(data, (0, 1, 3))\n",
    "    # Replicate data on tiles (1, 3)\n",
    "    t1 = tile_put_replicated(data, (1, 3))\n",
    "    return t0, t1\n",
    "\n",
    "\n",
    "t0, t1 = compute_fn(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4ea088f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor `t0` shape (3, 5) and tile mapping (0, 1, 3).\n",
      "Tensor `t1` shape (2, 3, 5) and tile mapping (1, 3).\n"
     ]
    }
   ],
   "source": [
    "# `t0` has the same shape, just sharded between tiles.\n",
    "print(f\"Tensor `t0` shape {t0.shape} and tile mapping {t0.tiles}.\")\n",
    "# `t1` has an additional replication axis.\n",
    "print(f\"Tensor `t1` shape {t1.shape} and tile mapping {t1.tiles}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49208128",
   "metadata": {},
   "source": [
    "`TileShardedArray` tensors support the basic array API, such as slicing and indexing. Note that an error will be raised if the slicing of a tensor would result into non-contiguous on tile shards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89793d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor `t3` shape (2, 3) and tile mapping (1, 3).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(DeviceArray([[0.41966587, 0.14291303, 0.10358273],\n",
       "              [0.1413389 , 0.7546814 , 0.7027907 ]], dtype=float32),\n",
       " array([[0.41966587, 0.14291303, 0.10358273],\n",
       "        [0.1413389 , 0.7546814 , 0.7027907 ]], dtype=float32))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t3 = t0[1:, 2:5]\n",
    "print(f\"Tensor `t3` shape {t3.shape} and tile mapping {t3.tiles}.\")\n",
    "# Extract the underlying tensor/array, or convert to a Numpy array\n",
    "t3.array, np.array(t3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e974a544",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "525771e3",
   "metadata": {},
   "source": [
    "## Tile map a vertex using Tessellate IPU\n",
    "\n",
    "Once tensors has been sharded across IPU tiles, users can map computation kernels (called IPU vertices) to these arrays. Tessellate IPU supports out of the box (part of) [JAX LAX operations](https://jax.readthedocs.io/en/latest/jax.lax.html) by mapping them to pre-existing Graphcore Poplar SDK optimized vertices, allowing to perform basic operations in a couple of lines.\n",
    "\n",
    "In the following example, we write a simple `broadcast_add` using Tessellate IPU. In this broadcast operation, the left hand term is sharded across a collection of tiles whereas the right hand term is broadcasted (i.e. replicated on all tiles). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f38e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: TileShardedArray(array=DeviceArray([[0.60848534, 1.0852832 , 0.90640026, 1.0351304 , 1.1440121 ],\n",
      "             [0.8170862 , 1.2950337 , 1.0239053 , 0.484455  , 1.3905538 ],\n",
      "             [0.43845683, 0.9908368 , 1.6852813 , 0.5040692 , 0.71705866]],            dtype=float32), tiles=(0, 1, 2))\n",
      "Excepted output: [[0.60848534 1.0852832  0.90640026 1.0351304  1.1440121 ]\n",
      " [0.8170862  1.2950337  1.0239053  0.484455   1.3905538 ]\n",
      " [0.43845683 0.9908368  1.6852813  0.5040692  0.71705866]]\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.lax\n",
    "import numpy as np\n",
    "\n",
    "from tessellate_ipu import tile_map, tile_put_replicated, tile_put_sharded\n",
    "\n",
    "lhs_data = np.random.rand(3, 5).astype(np.float32)\n",
    "rhs_data = np.random.rand(5).astype(np.float32)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def broadcast_add(lhs, rhs):\n",
    "    # Tiles to split the workload on.\n",
    "    tiles = tuple(range(len(lhs)))\n",
    "    # Shard lhs on tiles (0, 1, ..., N)\n",
    "    lhs = tile_put_sharded(lhs, tiles)\n",
    "    # Replicate rhs on tiles (0, 1, ..., N)\n",
    "    rhs = tile_put_replicated(rhs, tiles)\n",
    "\n",
    "    # Map Poplar optimized `add` vertex to the sharded data.\n",
    "    out = tile_map(jax.lax.add_p, lhs, rhs)\n",
    "    return out\n",
    "\n",
    "\n",
    "out = broadcast_add(lhs_data, rhs_data)\n",
    "\n",
    "print(\"Output:\", out)\n",
    "print(\"Excepted output:\", lhs_data + rhs_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cd7ae3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7d7a726",
   "metadata": {},
   "source": [
    "As seen above, `tile_map` will always return `TileShardedArray` objects, with tile mapping deduced from inputs. It will as well check that inputs are `TileShardedArray` instances. Since the `TileShardedArray` class insures that data is already sharded on IPU tiles in a contiguous form, `tile_map` has no performance overhead (i.e. no implicit on-tile-copy or tile exchange).\n",
    "\n",
    "**Note:** Tessellate IPU will always check that the tile mapping is consistent, and will raise an error if not. As the goal of Tessellate IPU is to provide a way to write performant & efficient algorithms directly in Python, implicit exchange between IPU tiles (or on-tile-copy) is not allowed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04d1c08f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tessellate error! Inconsistent tile mapping between input arrays: (0, 1, 2) vs (1, 2, 3).\n"
     ]
    }
   ],
   "source": [
    "@jax.jit\n",
    "def broadcast_add_error(lhs, rhs):\n",
    "    lhs = tile_put_sharded(lhs, range(len(lhs)))\n",
    "    rhs = tile_put_replicated(rhs, range(1, len(lhs) + 1))\n",
    "    out = tile_map(jax.lax.add_p, lhs, rhs)\n",
    "    return out\n",
    "\n",
    "\n",
    "# Raise `ValueError`: inconsistent tile mapping!\n",
    "try:\n",
    "    broadcast_add_error(lhs_data, rhs_data)\n",
    "except Exception as e:\n",
    "    print(\"Tessellate error!\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0d361d",
   "metadata": {},
   "source": [
    "Tessellate code written using standard JAX LAX primitives remains fully compatible with **other backends (CPU, GPU, TPU)**. `tile_put_sharded` is a no-op on other backends and `tile_put_replicated` is a simple `concatenate` of the input tensor. Finally `tile_map` is translated into a standard JAX `vmap` call.\n",
    "\n",
    "As a consequence, one can run the exact same function on JAX CPU backend:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e20f0dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: TileShardedArray(array=DeviceArray([[0.60848534, 1.0852832 , 0.90640026, 1.0351304 , 1.1440121 ],\n",
      "             [0.8170862 , 1.2950337 , 1.0239053 , 0.484455  , 1.3905538 ],\n",
      "             [0.43845683, 0.9908368 , 1.6852813 , 0.5040692 , 0.71705866]],            dtype=float32), tiles=(0, 1, 2)) on device: TFRT_CPU_0\n"
     ]
    }
   ],
   "source": [
    "# JIT function on CPU backend.\n",
    "broadcast_add_cpu = jax.jit(broadcast_add, device=jax.devices(\"cpu\")[0])\n",
    "# Running on CPU.\n",
    "out_cpu = broadcast_add_cpu(lhs_data, rhs_data)\n",
    "# Check data & device.\n",
    "print(f\"Output: {out_cpu} on device: {out_cpu.array.device()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00080b00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a186fa0",
   "metadata": {},
   "source": [
    "# Micro-benchmarking in Tessellate IPU\n",
    "\n",
    "When writing performant algorithms, micro-benchmarking is a recommended practice to ensure quick progress and no performance regression. Tessellate IPU is fully compatible with [Graphcore Popvision tools](https://www.graphcore.ai/developer/popvision-tools), but also provides a way to directly measure IPU hardware cycle count with the Python function `ipu_cycle_count`.\n",
    "\n",
    "**Note:** Cycle count is not available on the IPU model simulator, `ipu_cycle_count` will always return a zeroed tensor on the latter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3519b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start cycle count: TileShardedArray(array=DeviceArray([[391787498,         0],\n",
      "             [391787499,         0],\n",
      "             [391787499,         0]], dtype=uint32), tiles=(0, 1, 2)) (3, 2)\n",
      "End cycle count: TileShardedArray(array=DeviceArray([[391787852,         0],\n",
      "             [391787853,         0],\n",
      "             [391787853,         0]], dtype=uint32), tiles=(0, 1, 2)) (3, 2)\n"
     ]
    }
   ],
   "source": [
    "from tessellate_ipu import ipu_cycle_count\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def broadcast_add(lhs, rhs):\n",
    "    # Tiles to split the workload on.\n",
    "    tiles = tuple(range(len(lhs)))\n",
    "    # Shard lhs on tiles (0, 1, ..., N)\n",
    "    lhs = tile_put_sharded(lhs, tiles)\n",
    "    # Replicate rhs on tiles (0, 1, ..., N)\n",
    "    rhs = tile_put_replicated(rhs, tiles)\n",
    "\n",
    "    # Cycle count once inputs are sharded.\n",
    "    lhs, rhs, start = ipu_cycle_count(lhs, rhs)\n",
    "    # Map Poplar optimized `add` vertex to the sharded data.\n",
    "    out = tile_map(jax.lax.add_p, lhs, rhs)\n",
    "    # Cycle count after output is computed.\n",
    "    out, end = ipu_cycle_count(out)\n",
    "    return out, start, end\n",
    "\n",
    "\n",
    "_, start, end = broadcast_add(lhs_data, rhs_data)\n",
    "\n",
    "print(\"Start cycle count:\", start, start.shape)\n",
    "print(\"End cycle count:\", end, end.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9886bfcd",
   "metadata": {},
   "source": [
    "The function `ipu_cycle_count` returns raw cycle counts on every IPU-tile, directly measured by the hardware. Note that `ipu_cycle_count` takes input arguments and return them unchanged in order to provide control flow information to XLA and Poplar compilers (i.e. measure cycle counts after these tensors have been computed).\n",
    "\n",
    "\n",
    "Tessellate provides the raw values returned by IPU C++ intrinsics (https://docs.graphcore.ai/projects/poplar-api/en/2.4.0/ipu_builtins.html#get-count-l-from-csr), hence why `start` and `end` tensors have `uint32` as datatype. As shown below, the raw cycle count can be easily translated into time performance figures. Please note timing measured this way will differ massively from simple Python benchmarking of the function `broadcast_add`, as the latter will also include any JAX overhead, host to/from IPU communications and IPU tile exchange!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33142bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tile map `add` execution time (micro-seconds): 0.1918918918918919\n"
     ]
    }
   ],
   "source": [
    "def cycle_count_to_timing(start, end, ipu_device):\n",
    "    \"\"\"Convert raw cycle count into timing.\"\"\"\n",
    "    # Lower & upper bounds on cycle count.\n",
    "    start_min = np.min(start[:, 0])\n",
    "    end_max = np.max(end[:, 0])\n",
    "    cycle_count_diff = end_max - start_min\n",
    "    timing = cycle_count_diff / d.tile_clock_frequency\n",
    "    return timing\n",
    "\n",
    "\n",
    "timing = cycle_count_to_timing(start, end, d)\n",
    "print(\"Tile map `add` execution time (micro-seconds):\", timing * 1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fac07e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "31f5b427",
   "metadata": {},
   "source": [
    "### Cycle count & IPU tile parallelism\n",
    "\n",
    "Let's demonstrate IPU tile parallelism in a simple way by using hardware cycle count: `broadcast_add` performance should (roughly) independent of the input first axis size, as the workload is splitted uniformly between tiles. Note that timing can slighly varied depending on the number of tiles used as the latter are not synchronized when running compute independent workloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "31523a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark timing (us): [1.0043243243243243, 1.0075675675675677, 1.0178378378378379, 1.0475675675675675]\n"
     ]
    }
   ],
   "source": [
    "# Number of tiles to use.\n",
    "num_tiles_list = [4, 16, 128, 1024]\n",
    "\n",
    "\n",
    "def ipu_benchmark(num_tiles):\n",
    "    \"\"\"IPU benchmarking, splitting the workload over a collection of tiles.\"\"\"\n",
    "    # Workload size per tile.\n",
    "    wsize = 1024\n",
    "    lhs_data = np.random.rand(num_tiles, wsize).astype(np.float32)\n",
    "    rhs_data = np.random.rand(wsize).astype(np.float32)\n",
    "\n",
    "    _, start, end = broadcast_add(lhs_data, rhs_data)\n",
    "    timing = cycle_count_to_timing(start, end, d)\n",
    "    return timing\n",
    "\n",
    "\n",
    "benchmarks = []\n",
    "try:\n",
    "    benchmarks = [ipu_benchmark(N) * 1e6 for N in num_tiles_list]\n",
    "except:\n",
    "    print(\"Not working on using IPU model!\")\n",
    "\n",
    "# (Roughly) constant timing independently of the number of tiles used.\n",
    "print(\"Benchmark timing (us):\", benchmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331edbfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7bcb1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
